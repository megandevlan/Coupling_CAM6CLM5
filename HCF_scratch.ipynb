{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Goal: Start testing out how to compute HCF \n",
    "<b>Author:</b> Meg D. Fowler<br>\n",
    "<b>Date:</b> 29 Sept 2020 <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:56:27.035365Z",
     "start_time": "2020-09-30T17:56:25.600060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import comet as cm \n",
    "import numpy as np \n",
    "import xarray as xr \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import datetime \n",
    "import time \n",
    "\n",
    "# Plotting utils \n",
    "import matplotlib.pyplot as plt \n",
    "import cartopy\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.util\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in some data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What data do we need?</b> <br>\n",
    "Vertical profiles of: temperature (T), specific humidity (Q), geopotential height (zg in CESM2, Z3 in other runs), and pressure (P). <br>\n",
    "In addition, need lowest level temperature, specfic humidity, height, and pressure - so basically T2m, Q2m, PSfc, and 2m height. <br><br>\n",
    "<b>Units:</b><br>\n",
    "Temperature --> K <br>\n",
    "Height      --> m <br>\n",
    "Sp. Humidity -> kg/kg <br>\n",
    "Pressure    --> Pa  <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a focus on 1980 to test things out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:56:31.472214Z",
     "start_time": "2020-09-30T17:56:30.596130Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in profile of...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-712b54a28809>:11: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  Tpr['time'] = Tpr.indexes['time'].to_datetimeindex()\n",
      "<ipython-input-3-712b54a28809>:14: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  Zpr['time'] = Zpr.indexes['time'].to_datetimeindex()\n",
      "<ipython-input-3-712b54a28809>:17: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  Qpr['time'] = Qpr.indexes['time'].to_datetimeindex()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....T\n",
      ".....Z3\n",
      ".....Q\n",
      ".....P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-712b54a28809>:20: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  Ppr['time'] = Ppr.indexes['time'].to_datetimeindex()\n"
     ]
    }
   ],
   "source": [
    "dataDir = '/Users/mdfowler/Documents/Analysis/Coupling_initial/data/hrSim_CONUS/'\n",
    "\n",
    "Tpr_file = dataDir+'f.e21.FHIST_BGC.f09_f09_mg17.hourlyOutput.001.cam.h1.1980_hrT-UTCsel.nc'\n",
    "Zpr_file = dataDir+'f.e21.FHIST_BGC.f09_f09_mg17.hourlyOutput.001.cam.h1.1980_hrZ3-UTCsel.nc'\n",
    "Qpr_file = dataDir+'f.e21.FHIST_BGC.f09_f09_mg17.hourlyOutput.001.cam.h1.1980_hrQ-UTCsel.nc'\n",
    "Ppr_file = dataDir+'f.e21.FHIST_BGC.f09_f09_mg17.hourlyOutput.001.cam.h1.1979-1981_hrP-levels-UTCsel.nc'\n",
    "\n",
    "# ----------- Open files -------------\n",
    "print('Read in profile of...')\n",
    "Tpr = xr.open_dataset(Tpr_file, decode_times=True)\n",
    "Tpr['time'] = Tpr.indexes['time'].to_datetimeindex()\n",
    "print('.....T')\n",
    "Zpr = xr.open_dataset(Zpr_file, decode_times=True)\n",
    "Zpr['time'] = Zpr.indexes['time'].to_datetimeindex()\n",
    "print('.....Z3')\n",
    "Qpr = xr.open_dataset(Qpr_file, decode_times=True)\n",
    "Qpr['time'] = Qpr.indexes['time'].to_datetimeindex()\n",
    "print('.....Q')\n",
    "Ppr = xr.open_dataset(Ppr_file, decode_times=True)\n",
    "Ppr['time'] = Ppr.indexes['time'].to_datetimeindex()\n",
    "print('.....P')\n",
    "\n",
    "# ----------- Isolate 1980 in Ppr -----\n",
    "dates1980     = pd.DatetimeIndex(Tpr['time'].values)\n",
    "datesPpr      = pd.DatetimeIndex(Ppr['time'].values)\n",
    "iTimes        = np.where( (datesPpr>=(dates1980[0])) & (datesPpr<=dates1980[-1]) )[0]\n",
    "\n",
    "Ppr_sel       = Ppr.isel(time=iTimes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To make things a bit easier, assemble everything into a single dataframe, ordered with the surface level first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:56:36.414894Z",
     "start_time": "2020-09-30T17:56:33.609387Z"
    }
   },
   "outputs": [],
   "source": [
    "ds_Full = Tpr \n",
    "ds_Full['Qpr'] = (('time','lev','lat','lon'), Qpr.Q)\n",
    "ds_Full['Zpr'] = (('time','lev','lat','lon'), Zpr.Z3)\n",
    "ds_Full['Ppr'] = (('time','lev','lat','lon'), Ppr_sel.PRESSURE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:56:41.461436Z",
     "start_time": "2020-09-30T17:56:36.416515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pick out 12 UTC only \n",
    "ds_utc12 = ds_Full.where( ds_Full.UTC_hr==12.0 , drop=True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pick out test location/time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:56:41.469766Z",
     "start_time": "2020-09-30T17:56:41.463726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to dataframe for one point in space and time: \n",
    "iLat  = 10\n",
    "iLon  = 30 \n",
    "iTime = 10 \n",
    "\n",
    "DF = ds_utc12.isel(lat=iLat,lon=iLon,time=iTime).to_dataframe()\n",
    "\n",
    "# Flip order of levels so that surface comes first (required for function)\n",
    "DF = DF.reindex(index=DF.index[::-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:56:42.928723Z",
     "start_time": "2020-09-30T17:56:42.925649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "# Define length of dimensions \n",
    "nLev  = len(ds_utc12.lev)\n",
    "print(nLev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wondering about using f2py..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:56:44.526384Z",
     "start_time": "2020-09-30T17:56:44.522158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define variable names \n",
    "Tname = 'T'\n",
    "Qname = 'Qpr'\n",
    "Zname = 'Zpr'\n",
    "Pname = 'Ppr'\n",
    "\n",
    "# Profile starting at level above sfc\n",
    "tmp_in   = DF[Tname].values[1::]\n",
    "qhum_in  = DF[Qname].values[1::]\n",
    "hgt_in   = DF[Zname].values[1::]\n",
    "press_in = DF[Pname].values[1::]\n",
    "\n",
    "# Sfc values set as first level values \n",
    "t2m      = DF[Tname].values[0]\n",
    "q2m      = DF[Qname].values[0]\n",
    "h2m      = DF[Zname].values[0]\n",
    "psfc     = DF[Pname].values[0]\n",
    "\n",
    "# Number of levels to worry about in actual \"sounding\"\n",
    "nlev1 = nLev-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:47:59.540548Z",
     "start_time": "2020-09-30T17:47:59.516825Z"
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# Compute CTP-HiLow\n",
    "#--------------------------------\n",
    "from comet.metrics.fortran import hcf\n",
    "\n",
    "TBM, BCLH, BCLP, TDEF = hcf.hcf_vars_calc.hcfcalc(nlev1,\n",
    "                                                    tmp_in, \n",
    "                                                    press_in,\n",
    "                                                    qhum_in, \n",
    "                                                    hgt_in, \n",
    "                                                    t2m, \n",
    "                                                    psfc,\n",
    "                                                    q2m, \n",
    "                                                    h2m)\n",
    "\n",
    "\n",
    "# TBM  : *** buoyant mixing pot. temp (convective threshold) [K]\n",
    "# BCLH : *** height above ground of convective threshold [m]\n",
    "# BCLP : *** pressure of convective threshold level [Pa]\n",
    "# TDEF : *** potential temperature deficit need to initiate [K] \n",
    "\n",
    "# !! Note that evaluation metrics are not returned with this call. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317.9294128417969"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute using above approach (CoMeT f2py version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions \n",
    "nLat  = len(ds_utc12.lat)\n",
    "nLon  = len(ds_utc12.lon)\n",
    "nTime = len(ds_utc12.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable names \n",
    "Tname = 'T'\n",
    "Qname = 'Qpr'\n",
    "Zname = 'Zpr'\n",
    "Pname = 'Ppr'\n",
    "\n",
    "# Number of levels to worry about in actual \"sounding\"\n",
    "nLev  = len(ds_utc12.lev)\n",
    "nlev1 = nLev-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with lat 0 of 43 \n",
      "Done with lat 1 of 43 \n",
      "Done with lat 2 of 43 \n",
      "Done with lat 3 of 43 \n",
      "Done with lat 4 of 43 \n",
      "Done with lat 5 of 43 \n",
      "Done with lat 6 of 43 \n",
      "Done with lat 7 of 43 \n",
      "Done with lat 8 of 43 \n",
      "Done with lat 9 of 43 \n",
      "Done with lat 10 of 43 \n",
      "Done with lat 11 of 43 \n",
      "Done with lat 12 of 43 \n",
      "Done with lat 13 of 43 \n",
      "Done with lat 14 of 43 \n",
      "Done with lat 15 of 43 \n",
      "Done with lat 16 of 43 \n",
      "Done with lat 17 of 43 \n",
      "Done with lat 18 of 43 \n",
      "Done with lat 19 of 43 \n",
      "Done with lat 20 of 43 \n",
      "Done with lat 21 of 43 \n",
      "Done with lat 22 of 43 \n",
      "Done with lat 23 of 43 \n",
      "Done with lat 24 of 43 \n",
      "Done with lat 25 of 43 \n",
      "Done with lat 26 of 43 \n",
      "Done with lat 27 of 43 \n",
      "Done with lat 28 of 43 \n",
      "Done with lat 29 of 43 \n",
      "Done with lat 30 of 43 \n",
      "Done with lat 31 of 43 \n",
      "Done with lat 32 of 43 \n",
      "Done with lat 33 of 43 \n",
      "Done with lat 34 of 43 \n",
      "Done with lat 35 of 43 \n",
      "Done with lat 36 of 43 \n",
      "Done with lat 37 of 43 \n",
      "Done with lat 38 of 43 \n",
      "Done with lat 39 of 43 \n",
      "Done with lat 40 of 43 \n",
      "Done with lat 41 of 43 \n",
      "Done with lat 42 of 43 \n",
      "Time elapsed for all points and times: 1633.017 sec\n"
     ]
    }
   ],
   "source": [
    "# Define empty arrays to save things into \n",
    "TBM_all   = np.full([nTime,nLat,nLon], np.nan)\n",
    "BCLH_all  = np.full([nTime,nLat,nLon], np.nan)\n",
    "BCLP_all  = np.full([nTime,nLat,nLon], np.nan)\n",
    "TDEF_all  = np.full([nTime,nLat,nLon], np.nan)\n",
    "\n",
    "# Time how long this takes... \n",
    "t_start     = time.time()\n",
    "\n",
    "\n",
    "for iLat in range(nLat):\n",
    "    for iLon in range(nLon):\n",
    "        for iT in range(nTime):\n",
    "            \n",
    "            # Pick out specific point and time period \n",
    "            DF = ds_utc12.isel(lat=iLat,lon=iLon,time=iT).to_dataframe()\n",
    "            \n",
    "            # Flip order of levels so that surface comes first (required for function)\n",
    "            DF = DF.reindex(index=DF.index[::-1])\n",
    "            \n",
    "            # Compute HCF variables\n",
    "\n",
    "            TBM_all[iT,iLat,iLon], BCLH_all[iT,iLat,iLon], BCLP_all[iT,iLat,iLon], TDEF_all[iT,iLat,iLon] = hcf.hcf_vars_calc.hcfcalc(nlev1, \n",
    "                                                                                                    DF[Tname].values[1::], \n",
    "                                                                                                    DF[Pname].values[1::],\n",
    "                                                                                                    DF[Qname].values[1::], \n",
    "                                                                                                    DF[Zname].values[1::], \n",
    "                                                                                                    DF[Tname].values[0], \n",
    "                                                                                                    DF[Pname].values[0],\n",
    "                                                                                                    DF[Qname].values[0], \n",
    "                                                                                                    DF[Zname].values[0])\n",
    "    print('Done with lat %i of %i ' % (iLat, nLat))\n",
    "        \n",
    "\n",
    "print('Time elapsed for all points and times: %.3f sec' % (time.time() - t_start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save the array with TBM_all \n",
    "saveDir  = '/Users/mdfowler/Documents/Analysis/Coupling_initial/Coupling_CAM6CLM5/processed_data/'\n",
    "saveFile = 'f.e21.FHIST_BGC.f09_f09_mg17.hourlyOutput.001.HCF-TBM_1980_CONUS_12utc.p' \n",
    "\n",
    "pickle.dump( TBM_all, open( saveDir+saveFile, \"wb\" ), protocol=4 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save the array with TBM_all \n",
    "saveDir  = '/Users/mdfowler/Documents/Analysis/Coupling_initial/Coupling_CAM6CLM5/processed_data/'\n",
    "saveFile = 'f.e21.FHIST_BGC.f09_f09_mg17.hourlyOutput.001.HCF-BCLH_1980_CONUS_12utc.p' \n",
    "\n",
    "pickle.dump( BCLH_all, open( saveDir+saveFile, \"wb\" ), protocol=4 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save the array with TBM_all \n",
    "saveDir  = '/Users/mdfowler/Documents/Analysis/Coupling_initial/Coupling_CAM6CLM5/processed_data/'\n",
    "saveFile = 'f.e21.FHIST_BGC.f09_f09_mg17.hourlyOutput.001.HCF-BCLP_1980_CONUS_12utc.p' \n",
    "\n",
    "pickle.dump( BCLP_all, open( saveDir+saveFile, \"wb\" ), protocol=4 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save the array with TBM_all \n",
    "saveDir  = '/Users/mdfowler/Documents/Analysis/Coupling_initial/Coupling_CAM6CLM5/processed_data/'\n",
    "saveFile = 'f.e21.FHIST_BGC.f09_f09_mg17.hourlyOutput.001.HCF-TDEF_1980_CONUS_12utc.p' \n",
    "\n",
    "pickle.dump( TDEF_all, open( saveDir+saveFile, \"wb\" ), protocol=4 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start attempt to compute from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:02:41.127679Z",
     "start_time": "2020-09-30T17:02:41.123614Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define variable names \n",
    "Tname = 'T'\n",
    "Qname = 'Qpr'\n",
    "Zname = 'Zpr'\n",
    "Pname = 'Ppr'\n",
    "\n",
    "# Profile starting at level above sfc\n",
    "tmp_in   = DF[Tname].values[1::]\n",
    "qhum_in  = DF[Qname].values[1::]\n",
    "hgt_in   = DF[Zname].values[1::]\n",
    "press_in = DF[Pname].values[1::]\n",
    "\n",
    "# Sfc values set as first level values \n",
    "t2m      = DF[Tname].values[0]\n",
    "q2m      = DF[Qname].values[0]\n",
    "h2m      = DF[Zname].values[0]\n",
    "psfc     = DF[Pname].values[0]\n",
    "\n",
    "# Number of levels to worry about in actual \"sounding\"\n",
    "nlev1 = nLev-1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:02:41.981548Z",
     "start_time": "2020-09-30T17:02:41.977900Z"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "#    Set constants \n",
    "# -----------------------------------------------\n",
    "p_ref  = 1e5 \n",
    "Lv     = 2.5e6 \n",
    "cp     = 1005.7\n",
    "R_cp   = 287.04/1005.7\n",
    "\n",
    "grav   = 9.81 \n",
    "Rd     = 287.04\n",
    "pi     = np.pi\n",
    "cp_g   = cp/grav\n",
    "Lv_g   = Lv/grav\n",
    "r2d    = 180./pi\n",
    "\n",
    "by100  = 1e2\n",
    "t0     = 273.15, \n",
    "ep     = 0.622\n",
    "es0    = 6.11\n",
    "a      = 17.269\n",
    "b      = 35.86\n",
    "onemep = 1.0 - ep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:05:21.333928Z",
     "start_time": "2020-09-30T17:05:21.326525Z"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "#    Initiate empty arrays \n",
    "# -----------------------------------------------\n",
    "shdef = np.full([nlev1+1], np.nan)\n",
    "lhdef = np.full([nlev1+1], np.nan)\n",
    "eadv  = np.full([nlev1+1], np.nan)\n",
    "rhoh  = np.full([nlev1+1], np.nan)\n",
    "pbar  = np.full([nlev1+1], np.nan)\n",
    "qdef  = np.full([nlev1+1], np.nan)\n",
    "qmix  = np.full([nlev1+1], np.nan)\n",
    "\n",
    "qsat   = np.full([nlev1+1], np.nan)\n",
    "dpress = np.full([nlev1+1], np.nan) \n",
    "qbar   = np.full([nlev1+1], np.nan)\n",
    "logp   = np.full([nlev1+1], np.nan)\n",
    "hbar   = np.full([nlev1+1], np.nan)\n",
    "tbar   = np.full([nlev1+1], np.nan)\n",
    "tmp_k  = np.full([nlev1+1], np.nan)\n",
    "press  = np.full([nlev1+1], np.nan)\n",
    "pot_k  = np.full([nlev1+1], np.nan)\n",
    "hgt    = np.full([nlev1+1], np.nan)\n",
    "qhum   = np.full([nlev1+1], np.nan)\n",
    "\n",
    "pot_diff = np.full([nlev1+1], np.nan)\n",
    "eadv_0   = np.full([nlev1+1], np.nan)\n",
    "xaxis    = np.full([nlev1+1], np.nan)\n",
    "xaxis1   = np.full([nlev1+1], np.nan)\n",
    "yaxis    = np.full([nlev1+1], np.nan)\n",
    "yaxis1   = np.full([nlev1+1], np.nan)\n",
    "integral = np.full([nlev1+1], np.nan)\n",
    "below    = np.full([nlev1+1], np.nan)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:09:39.965400Z",
     "start_time": "2020-09-30T17:09:39.962122Z"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "#    Store temp working arrays and initialize \n",
    "# -----------------------------------------------\n",
    "nlev      = nlev1+1 \n",
    "\n",
    "tmp_k[1:] = tmp_in \n",
    "hgt[1:]   = hgt_in\n",
    "qhum[1:]  = qhum_in\n",
    "press[1:] = press_in \n",
    "\n",
    "tmp_k[0] = t2m \n",
    "hgt[0]   = h2m\n",
    "qhum[0]  = q2m \n",
    "press[0] = psfc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:09:41.164455Z",
     "start_time": "2020-09-30T17:09:41.160914Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----- Run a few checks on pressure (but most of this is unfortunately on the user ) ------- \n",
    "\n",
    "# Check that pressure levels aren't greater than the surface pressure \n",
    "iProblem = np.where(press[1:] >= psfc)[0]\n",
    "\n",
    "if len(iProblem)>0:\n",
    "    print('***** ERROR: lowest pressure level > surface pressure *****')\n",
    "\n",
    "# Check ordering of Plev: \n",
    "if press[0]<press[-1]:\n",
    "    print('***** ERROR: pressure levels should be reversed *****') \n",
    "\n",
    "# Check units of pressure \n",
    "if psfc<=2000.0: \n",
    "    print('**** ERROR: pressures should be in Pa, not hPa *****') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In much of code below, I have the calculation in .f90 from CoMeT ncl script commented out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T17:32:38.026822Z",
     "start_time": "2020-09-30T17:32:38.019357Z"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "#    Compute column potential temperature \n",
    "# -----------------------------------------------\n",
    "pot_k = tmp_k * (p_ref/press)**(R_cp)\n",
    "\n",
    "# -----------------------------------------------\n",
    "#    Ignore missing data levels when calculating midpoints \n",
    "#     (shouldn't be an issue for model data)\n",
    "# -----------------------------------------------\n",
    "hbar = hgt\n",
    "pbar = press\n",
    "tbar = tmp_k\n",
    "\n",
    "# -----------------------------------------------\n",
    "#    Compute middle layer specific humidity average [kg/kg]\n",
    "#    1st layer = the 2m sp. humidity above, then layer averages above \n",
    "# -----------------------------------------------\n",
    "qbar = qhum \n",
    "qbar[1:nlev] = ( (qhum[1:nlev]*np.log(press[1:nlev]) + qhum[0:nlev-1]*np.log(press[0:nlev-1]) )  / \n",
    "                  np.log(press[1:nlev]*press[0:nlev-1]))\n",
    "# qbar(2:nlev)    = ((qhum(2:nlev  )*log(press(2:nlev  ))  + &\n",
    "#                                   qhum(1:nlev-1)*log(press(1:nlev-1))) / &\n",
    "#                                   log(press(2:nlev)* press(1:nlev-1)))\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "#    Compute pressure difference of each layer \n",
    "# -----------------------------------------------\n",
    "if dpress[0]<=0: \n",
    "    dpress[0] = 1.     # Set to 1 Pa because h2m is likely zero \n",
    "else:\n",
    "    dpress[0]   = (psfc / (Rd * t2m * ((1. + (q2m/ep)) / (1. + q2m)) )) * grav * h2m \n",
    "    #dpress(1)  =  (psfc / (Rd * t2m * ((1. + (q2m/ep)) / (1. + q2m)) )) * grav * h2m\n",
    "    \n",
    "# Model data shouldn't have any missing, so not using this line:\n",
    "#    where( pbar(1:nlev-1).ne.missing  .and.  pbar(2:nlev).ne.missing )\n",
    "dpress[1:nlev] = press[0:nlev-1] - press[1:nlev]\n",
    "\n",
    "# -----------------------------------------------\n",
    "#    Compute log pressure ot linearize it for slope calculation \n",
    "# -----------------------------------------------\n",
    "logp = np.log(pbar)\n",
    "\n",
    "# -----------------------------------------------\n",
    "#    Compute mixed layer sp. humidity and column density [kg/kg]\n",
    "# -----------------------------------------------\n",
    "qmix = qbar * dpress/grav\n",
    "rhoh = dpress/grav\n",
    "\n",
    "for izz in range(nlev-1):\n",
    "    zz = izz+1    # .f90 is: do zz = 2, nlev; so going to increase index by one \n",
    "    if (np.isfinite(qmix[zz]) & np.isfinite(qmix[zz-1])): \n",
    "        qmix[zz] = qmix[zz-1] + qmix[zz]\n",
    "        \n",
    "    if (np.isfinite(rhoh[zz]) & np.isfinite(rhoh[zz-1]) ):\n",
    "        rhoh[zz] = rhoh[zz-1] + rhoh[zz]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
